#!/usr/bin/env python3
"""
è¯­éŸ³LLMå¤„ç†å™¨ - å°†è¯­éŸ³è¯†åˆ«ç»“æœå‘é€åˆ°LLMè¿›è¡Œå¤„ç†
"""

import threading
import queue
import time
from typing import Optional, Callable, Dict, Any, List
from dataclasses import dataclass

from .GeminiLLM import GeminiClient, GeminiResponse


@dataclass
class VoiceLLMRequest:
    """è¯­éŸ³LLMè¯·æ±‚æ•°æ®"""
    text: str
    timestamp: float
    request_id: str
    system_prompt: Optional[str] = None
    user_context: Optional[Dict[str, Any]] = None


@dataclass
class VoiceLLMResponse:
    """è¯­éŸ³LLMå“åº”æ•°æ®"""
    request_id: str
    original_text: str
    llm_response: str
    timestamp: float
    processing_time: float
    success: bool
    error: Optional[str] = None


class VoiceLLMHandler:
    """è¯­éŸ³LLMå¤„ç†å™¨ç±»"""
    
    def __init__(self, config=None):
        """
        åˆå§‹åŒ–è¯­éŸ³LLMå¤„ç†å™¨
        
        Args:
            config: é…ç½®ç®¡ç†å™¨å®ä¾‹
        """
        self.config = config
        self.gemini_client: Optional[GeminiClient] = None
        
        # è¯·æ±‚é˜Ÿåˆ—å’Œå“åº”å›è°ƒ
        self.request_queue = queue.Queue()
        self.response_callback: Optional[Callable[[VoiceLLMResponse], None]] = None
        
        # å¤„ç†çº¿ç¨‹
        self.processing_thread: Optional[threading.Thread] = None
        self.is_running = False
        
        # å¯¹è¯å†å² (æ”¯æŒå¤šè½®å¯¹è¯)
        self.conversation_history: List[Dict[str, str]] = []
        self.max_history_length = 10  # ä¿ç•™æœ€è¿‘10è½®å¯¹è¯
        
        # é»˜è®¤ç³»ç»Ÿæç¤ºè¯
        self.default_system_prompt = """ä½ æ˜¯ä¸€ä¸ªå‹å–„ã€æœ‰ç”¨çš„AIåŠ©æ‰‹ã€‚è¯·ç”¨ç®€æ´ã€è‡ªç„¶çš„è¯­è¨€å›å¤ç”¨æˆ·çš„é—®é¢˜ã€‚
å¦‚æœç”¨æˆ·è¯´çš„æ˜¯æ—¥è¯­ï¼Œè¯·ç”¨æ—¥è¯­å›å¤ï¼›å¦‚æœæ˜¯ä¸­æ–‡ï¼Œè¯·ç”¨ä¸­æ–‡å›å¤ï¼›å¦‚æœæ˜¯è‹±è¯­ï¼Œè¯·ç”¨è‹±è¯­å›å¤ã€‚
ä¿æŒå›å¤ç®€çŸ­ä½†æœ‰ç”¨ï¼Œé€‚åˆè¯­éŸ³å¯¹è¯çš„åœºæ™¯ã€‚"""
        
        # åˆå§‹åŒ–LLMå®¢æˆ·ç«¯
        self._init_llm_client()
        
        print("âœ… è¯­éŸ³LLMå¤„ç†å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def _init_llm_client(self):
        """åˆå§‹åŒ–LLMå®¢æˆ·ç«¯"""
        try:
            if not self.config:
                print("âš ï¸ æ²¡æœ‰é…ç½®ç®¡ç†å™¨ï¼Œæ— æ³•åˆå§‹åŒ–LLMå®¢æˆ·ç«¯")
                return
            
            # ä»é…ç½®è·å–API Key
            api_key = self.config.get('LLM', 'gemini_api_key')
            if not api_key:
                print("âš ï¸ æœªé…ç½®Gemini API Keyï¼ŒLLMåŠŸèƒ½ä¸å¯ç”¨")
                return
            
            # è·å–æ¨¡å‹é…ç½®
            model = self.config.get('LLM', 'gemini_model', 'gemini-1.5-flash')
            
            # åˆ›å»ºå®¢æˆ·ç«¯
            self.gemini_client = GeminiClient(
                api_key=api_key,
                model=model,
                config=self.config
            )
            
            # æµ‹è¯•è¿æ¥
            if self.gemini_client.test_connection():
                print("âœ… Geminiå®¢æˆ·ç«¯è¿æ¥æµ‹è¯•æˆåŠŸ")
            else:
                print("âŒ Geminiå®¢æˆ·ç«¯è¿æ¥æµ‹è¯•å¤±è´¥")
                self.gemini_client = None
                
        except Exception as e:
            print(f"âŒ åˆå§‹åŒ–LLMå®¢æˆ·ç«¯å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            self.gemini_client = None
    
    def set_response_callback(self, callback: Callable[[VoiceLLMResponse], None]):
        """
        è®¾ç½®å“åº”å›è°ƒå‡½æ•°
        
        Args:
            callback: å“åº”å›è°ƒå‡½æ•°ï¼Œæ¥æ”¶VoiceLLMResponseå‚æ•°
        """
        self.response_callback = callback
        print("âœ… å·²è®¾ç½®å“åº”å›è°ƒå‡½æ•°")
    
    def start_processing(self):
        """å¼€å§‹å¤„ç†è¯·æ±‚é˜Ÿåˆ—"""
        if self.is_running:
            print("âš ï¸ å¤„ç†å™¨å·²åœ¨è¿è¡Œä¸­")
            return
        
        if not self.gemini_client:
            print("âŒ LLMå®¢æˆ·ç«¯æœªåˆå§‹åŒ–ï¼Œæ— æ³•å¼€å§‹å¤„ç†")
            return
        
        self.is_running = True
        self.processing_thread = threading.Thread(target=self._processing_loop, daemon=True)
        self.processing_thread.start()
        print("âœ… è¯­éŸ³LLMå¤„ç†å™¨å·²å¯åŠ¨")
    
    def stop_processing(self):
        """åœæ­¢å¤„ç†è¯·æ±‚é˜Ÿåˆ—"""
        if not self.is_running:
            return
        
        self.is_running = False
        if self.processing_thread:
            self.processing_thread.join(timeout=5.0)
        print("â¹ï¸ è¯­éŸ³LLMå¤„ç†å™¨å·²åœæ­¢")
    
    def _processing_loop(self):
        """å¤„ç†å¾ªç¯"""
        while self.is_running:
            try:
                # ä»é˜Ÿåˆ—è·å–è¯·æ±‚
                request = self.request_queue.get(timeout=1.0)
                
                # å¤„ç†è¯·æ±‚
                response = self._process_request(request)
                
                # è°ƒç”¨å›è°ƒå‡½æ•°
                if self.response_callback:
                    try:
                        self.response_callback(response)
                    except Exception as e:
                        print(f"âŒ å“åº”å›è°ƒå‡½æ•°æ‰§è¡Œå¤±è´¥: {e}")
                
                # æ ‡è®°é˜Ÿåˆ—ä»»åŠ¡å®Œæˆ
                self.request_queue.task_done()
                
            except queue.Empty:
                continue
            except Exception as e:
                print(f"âŒ å¤„ç†å¾ªç¯å¼‚å¸¸: {e}")
                import traceback
                traceback.print_exc()
    
    def _process_request(self, request: VoiceLLMRequest) -> VoiceLLMResponse:
        """
        å¤„ç†å•ä¸ªè¯·æ±‚
        
        Args:
            request: è¯­éŸ³LLMè¯·æ±‚
            
        Returns:
            å¤„ç†å“åº”
        """
        start_time = time.time()
        
        try:
            print(f"ğŸ¤– å¤„ç†è¯­éŸ³LLMè¯·æ±‚: {request.text[:50]}...")
            
            if not self.gemini_client:
                return VoiceLLMResponse(
                    request_id=request.request_id,
                    original_text=request.text,
                    llm_response="",
                    timestamp=time.time(),
                    processing_time=time.time() - start_time,
                    success=False,
                    error="LLMå®¢æˆ·ç«¯æœªåˆå§‹åŒ–"
                )
            
            # å†³å®šä½¿ç”¨å“ªç§å¤„ç†æ–¹å¼
            if len(self.conversation_history) == 0:
                # é¦–æ¬¡å¯¹è¯ï¼Œä½¿ç”¨generate_content
                system_prompt = request.system_prompt or self.default_system_prompt
                llm_response = self.gemini_client.generate_content(
                    prompt=request.text,
                    system_prompt=system_prompt
                )
            else:
                # å¤šè½®å¯¹è¯ï¼Œä½¿ç”¨chat
                llm_response = self.gemini_client.chat(
                    message=request.text,
                    conversation_history=self.conversation_history
                )
            
            # å¤„ç†å“åº”
            if llm_response.error:
                return VoiceLLMResponse(
                    request_id=request.request_id,
                    original_text=request.text,
                    llm_response="",
                    timestamp=time.time(),
                    processing_time=time.time() - start_time,
                    success=False,
                    error=llm_response.error
                )
            
            response_text = llm_response.text
            
            # æ›´æ–°å¯¹è¯å†å²
            self._update_conversation_history(request.text, response_text)
            
            processing_time = time.time() - start_time
            print(f"âœ… LLMå“åº”å®Œæˆï¼Œè€—æ—¶: {processing_time:.2f}ç§’")
            
            return VoiceLLMResponse(
                request_id=request.request_id,
                original_text=request.text,
                llm_response=response_text,
                timestamp=time.time(),
                processing_time=processing_time,
                success=True
            )
            
        except Exception as e:
            print(f"âŒ å¤„ç†è¯·æ±‚å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            
            return VoiceLLMResponse(
                request_id=request.request_id,
                original_text=request.text,
                llm_response="",
                timestamp=time.time(),
                processing_time=time.time() - start_time,
                success=False,
                error=f"å¤„ç†å¼‚å¸¸: {str(e)}"
            )
    
    def _update_conversation_history(self, user_text: str, assistant_text: str):
        """
        æ›´æ–°å¯¹è¯å†å²
        
        Args:
            user_text: ç”¨æˆ·è¾“å…¥
            assistant_text: åŠ©æ‰‹å›å¤
        """
        # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯
        self.conversation_history.append({
            "role": "user",
            "text": user_text
        })
        
        # æ·»åŠ åŠ©æ‰‹å›å¤
        self.conversation_history.append({
            "role": "assistant",
            "text": assistant_text
        })
        
        # ä¿æŒå†å²é•¿åº¦é™åˆ¶
        while len(self.conversation_history) > self.max_history_length * 2:  # *2 å› ä¸ºæ¯è½®æœ‰ç”¨æˆ·å’ŒåŠ©æ‰‹ä¸¤æ¡æ¶ˆæ¯
            self.conversation_history.pop(0)
    
    def submit_voice_text(self, text: str, system_prompt: Optional[str] = None, 
                         user_context: Optional[Dict[str, Any]] = None) -> str:
        """
        æäº¤è¯­éŸ³æ–‡æœ¬è¿›è¡ŒLLMå¤„ç†
        
        Args:
            text: è¯†åˆ«å‡ºçš„è¯­éŸ³æ–‡æœ¬
            system_prompt: è‡ªå®šä¹‰ç³»ç»Ÿæç¤ºè¯
            user_context: ç”¨æˆ·ä¸Šä¸‹æ–‡ä¿¡æ¯
            
        Returns:
            è¯·æ±‚ID
        """
        if not text.strip():
            print("âš ï¸ ç©ºæ–‡æœ¬ï¼Œè·³è¿‡LLMå¤„ç†")
            return ""
        
        if not self.is_running:
            print("âš ï¸ å¤„ç†å™¨æœªè¿è¡Œï¼Œæ— æ³•æäº¤è¯·æ±‚")
            return ""
        
        # ç”Ÿæˆè¯·æ±‚ID
        request_id = f"req_{int(time.time() * 1000)}_{hash(text) % 10000}"
        
        # åˆ›å»ºè¯·æ±‚
        request = VoiceLLMRequest(
            text=text,
            timestamp=time.time(),
            request_id=request_id,
            system_prompt=system_prompt,
            user_context=user_context
        )
        
        # æ·»åŠ åˆ°é˜Ÿåˆ—
        try:
            self.request_queue.put(request, timeout=1.0)
            print(f"ğŸ“ å·²æäº¤è¯­éŸ³æ–‡æœ¬åˆ°LLM: {text[:50]}... (ID: {request_id})")
            return request_id
        except queue.Full:
            print("âŒ è¯·æ±‚é˜Ÿåˆ—å·²æ»¡ï¼Œæ— æ³•æäº¤")
            return ""
    
    def clear_conversation_history(self):
        """æ¸…ç©ºå¯¹è¯å†å²"""
        self.conversation_history.clear()
        print("ğŸ—‘ï¸ å·²æ¸…ç©ºå¯¹è¯å†å²")
    
    def get_queue_size(self) -> int:
        """è·å–å½“å‰é˜Ÿåˆ—å¤§å°"""
        return self.request_queue.qsize()
    
    def is_client_ready(self) -> bool:
        """æ£€æŸ¥LLMå®¢æˆ·ç«¯æ˜¯å¦å°±ç»ª"""
        return self.gemini_client is not None
    
    def update_api_key(self, api_key: str):
        """
        æ›´æ–°API Key
        
        Args:
            api_key: æ–°çš„API Key
        """
        try:
            if not api_key.strip():
                print("âš ï¸ ç©ºçš„API Key")
                return False
            
            # åœæ­¢å½“å‰å¤„ç†
            was_running = self.is_running
            if was_running:
                self.stop_processing()
            
            # é‡æ–°åˆå§‹åŒ–å®¢æˆ·ç«¯
            model = self.config.get('LLM', 'gemini_model', 'gemini-1.5-flash') if self.config else 'gemini-1.5-flash'
            self.gemini_client = GeminiClient(
                api_key=api_key,
                model=model,
                config=self.config
            )
            
            # æµ‹è¯•è¿æ¥
            if self.gemini_client.test_connection():
                print("âœ… API Keyæ›´æ–°æˆåŠŸï¼Œè¿æ¥æµ‹è¯•é€šè¿‡")
                
                # æ¢å¤å¤„ç†ï¼ˆå¦‚æœä¹‹å‰åœ¨è¿è¡Œï¼‰
                if was_running:
                    self.start_processing()
                
                return True
            else:
                print("âŒ API Keyæ›´æ–°å¤±è´¥ï¼Œè¿æ¥æµ‹è¯•ä¸é€šè¿‡")
                self.gemini_client = None
                return False
                
        except Exception as e:
            print(f"âŒ æ›´æ–°API Keyå¤±è´¥: {e}")
            self.gemini_client = None
            return False